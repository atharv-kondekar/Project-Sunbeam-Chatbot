COURSE: Apache Spark Mastery - Data Engineering with PySpark
CATEGORY: Data Engineering

DURATION: 50 Hours
FEES: â‚¹14900 (Incl. GST)
SCHEDULE: 16-Aug-2025 to 17-Sep-2025
TIMINGS: 7:00 PM to 9:00 PM
MODE: Mon - Sat

TARGET AUDIENCE:
- Data Engineers
- Python Developers
- Freshers with basic Python

PREREQUISITES:
- Python basics: functions, collections, pandas
- SQL queries: CRUD, joins, analytical queries
- Nice to have: Linux OS basics, Hadoop/Hive awareness

SYLLABUS MODULES:

Spark Core & Architecture:
- Distributed computing fundamentals, DAG scheduler, cluster managers
- Lazy evaluation, RDD lineage, Spark 4.x features
- Adaptive Query Execution (AQE), Catalyst optimizer

PySpark DataFrames & SQL:
- Complex data types: JSON, arrays, maps
- Window functions, UDFs/Pandas UDFs
- Spark SQL: temp views, catalog, Hive metastore

Incremental Processing & Streaming:
- Structured streaming concepts, watermarking, event-time, state mgmt
- Kafka integration (source/sink)
- Delta Lake: ACID transactions, schema evolution

Performance & Optimization:
- Catalyst internals: logical/physical plans
- `explain()`, predicate pushdown, partition pruning
- Parquet/Delta formats, resource tuning fundamentals

Databricks & Lakehouse:
- Databricks workspace, DBFS, clusters, Unity Catalog basics
- Delta table versioning, schema history

Kafka Integration:
- Brokers, partitions, consumer groups
- Structured streaming with Spark + Kafka

Spark ML (Intro):
- MLlib basics, pipelines, transformers vs estimators
- Simple regression/classification demo

CAPSTONE PROJECT:
- End-to-end pipeline for IoT or retail dataset
- Batch + streaming + Delta Lake + PySpark

OUTCOMES:
- Build scalable ETL pipelines
- Optimize Spark jobs using Catalyst insights
- Integrate Spark with Kafka for streaming data
- Work with Delta Lake for ACID data lakes
